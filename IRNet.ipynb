{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IRNet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO67Yf+/pTeooG35yWvlY5W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surajsrivathsa/NL2SQL/blob/main/IRNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njCn-ILPk-xV"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdG1EOCmYIEP"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smjagKeqb4WQ",
        "outputId": "7173bcb4-64c5-4182-a11e-581e328a33f8"
      },
      "source": [
        "# !pip install nltk==3.4\n",
        "!pip install pattern\n",
        "!pip install pytorch-pretrained-bert==0.5.1\n",
        "# !pip install tqdm==4.31.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pattern in /usr/local/lib/python3.7/dist-packages (3.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pattern) (3.2.5)\n",
            "Requirement already satisfied: mysqlclient in /usr/local/lib/python3.7/dist-packages (from pattern) (2.0.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pattern) (4.2.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pattern) (2.23.0)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.7/dist-packages (from pattern) (6.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.19.5)\n",
            "Requirement already satisfied: backports.csv in /usr/local/lib/python3.7/dist-packages (from pattern) (1.0.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pattern) (0.16.0)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.7/dist-packages (from pattern) (0.8.10)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from pattern) (4.6.3)\n",
            "Requirement already satisfied: cherrypy in /usr/local/lib/python3.7/dist-packages (from pattern) (18.6.0)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.7/dist-packages (from pattern) (20201018)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->pattern) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2.10)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.7/dist-packages (from feedparser->pattern) (1.0.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (8.7.0)\n",
            "Requirement already satisfied: portend>=2.1.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (2.7.1)\n",
            "Requirement already satisfied: zc.lockfile in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (2.0)\n",
            "Requirement already satisfied: cheroot>=8.2.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (8.5.2)\n",
            "Requirement already satisfied: jaraco.collections in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (3.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->pattern) (2.3.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->pattern) (3.4.7)\n",
            "Requirement already satisfied: tempora>=1.8 in /usr/local/lib/python3.7/dist-packages (from portend>=2.1.1->cherrypy->pattern) (4.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zc.lockfile->cherrypy->pattern) (56.0.0)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.7/dist-packages (from cheroot>=8.2.1->cherrypy->pattern) (3.3.0)\n",
            "Requirement already satisfied: jaraco.text in /usr/local/lib/python3.7/dist-packages (from jaraco.collections->cherrypy->pattern) (3.5.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.7/dist-packages (from jaraco.collections->cherrypy->pattern) (3.2.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->pattern) (1.14.5)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2018.9)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->pattern) (2.20)\n",
            "Requirement already satisfied: pytorch-pretrained-bert==0.5.1 in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert==0.5.1) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert==0.5.1) (1.17.68)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert==0.5.1) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert==0.5.1) (4.41.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert==0.5.1) (1.8.1+cu101)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert==0.5.1) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert==0.5.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert==0.5.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert==0.5.1) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert==0.5.1) (0.4.2)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.68 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert==0.5.1) (1.20.68)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert==0.5.1) (0.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert==0.5.1) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.68->boto3->pytorch-pretrained-bert==0.5.1) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.68->boto3->pytorch-pretrained-bert==0.5.1) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bko9vQG7b4ZI"
      },
      "source": [
        "import nltk\n",
        "import tqdm\n",
        "import pattern"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVUoRFPBTlfD",
        "outputId": "cbbb6087-6373-40b8-e210-9b16e4b8441e"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOLL8vpzb4cJ"
      },
      "source": [
        "import torch\n",
        "import random\n",
        "import argparse\n",
        "import copy\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqHNJCtKkgSQ",
        "outputId": "bdb2bc68-7c0f-4142-9dc1-184c63538d1e"
      },
      "source": [
        "import os, sys, glob\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7DKr2Web4e_"
      },
      "source": [
        "# !pip install git+https:https://github.com/microsoft/IRNet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTBFWjdVn2Rr"
      },
      "source": [
        "base_path = \"/content/drive/My Drive/NL2SQL/Models/IRNet-master\"\n",
        "data_path = \"/content/drive/My Drive/NL2SQL/Models/IRNet-master/data\"\n",
        "model_path = \"/content/drive/My Drive/NL2SQL/Models/IRNet-master/saved_model\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ2lob0dReqr"
      },
      "source": [
        "sys.path.append(base_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsBtsbBffF1v"
      },
      "source": [
        "from src import args as arg\n",
        "from src import utils\n",
        "from src.models.model import IRNet\n",
        "from src.rule import semQL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yowEr0IfF5T"
      },
      "source": [
        "def init_arg_parser():\n",
        "    print(\"inside argparse\")\n",
        "    arg_parser = argparse.ArgumentParser()\n",
        "    arg_parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
        "    arg_parser.add_argument('--seed', default=90, type=int, help='random seed')\n",
        "    arg_parser.add_argument('--cuda', action='store_true', help='use gpu',  default=True)\n",
        "    arg_parser.add_argument('--lr_scheduler', action='store_true', help='use learning rate scheduler')\n",
        "    arg_parser.add_argument('--lr_scheduler_gammar', default=0.5, type=float, help='decay rate of learning rate scheduler')\n",
        "    arg_parser.add_argument('--column_pointer', action='store_true', help='use column pointer')\n",
        "    arg_parser.add_argument('--loss_epoch_threshold', default=20, type=int, help='loss epoch threshold')\n",
        "    arg_parser.add_argument('--sketch_loss_coefficient', default=1.0, type=float, help='sketch loss coefficient')\n",
        "    arg_parser.add_argument('--sentence_features', action='store_true', help='use sentence features')\n",
        "    arg_parser.add_argument('--model_name', choices=['transformer', 'rnn', 'table', 'sketch'], default='rnn',\n",
        "                            help='model name')\n",
        "\n",
        "    arg_parser.add_argument('--lstm', choices=['lstm', 'lstm_with_dropout', 'parent_feed'], default='lstm')\n",
        "\n",
        "    arg_parser.add_argument('--load_model', default=model_path+\"/IRNet_pretrained.model\", type=str, help='load a pre-trained model')\n",
        "    arg_parser.add_argument('--glove_embed_path', default=data_path+\"/\"+\"glove.42B.300d.txt\", type=str)\n",
        "\n",
        "    arg_parser.add_argument('--batch_size', default=64, type=int, help='batch size')\n",
        "    arg_parser.add_argument('--beam_size', default=5, type=int, help='beam size for beam search')\n",
        "    arg_parser.add_argument('--embed_size', default=300, type=int, help='size of word embeddings')\n",
        "    arg_parser.add_argument('--col_embed_size', default=300, type=int, help='size of word embeddings')\n",
        "\n",
        "    arg_parser.add_argument('--action_embed_size', default=128, type=int, help='size of word embeddings')\n",
        "    arg_parser.add_argument('--type_embed_size', default=128, type=int, help='size of word embeddings')\n",
        "    arg_parser.add_argument('--hidden_size', default=300, type=int, help='size of LSTM hidden states')\n",
        "    arg_parser.add_argument('--att_vec_size', default=300, type=int, help='size of attentional vector')\n",
        "    arg_parser.add_argument('--dropout', default=0.3, type=float, help='dropout rate')\n",
        "    arg_parser.add_argument('--word_dropout', default=0.2, type=float, help='word dropout rate')\n",
        "\n",
        "    # readout layer\n",
        "    arg_parser.add_argument('--no_query_vec_to_action_map', default=False, action='store_true')\n",
        "    arg_parser.add_argument('--readout', default='identity', choices=['identity', 'non_linear'])\n",
        "    arg_parser.add_argument('--query_vec_to_action_diff_map', default=False, action='store_true')\n",
        "\n",
        "\n",
        "    arg_parser.add_argument('--column_att', choices=['dot_prod', 'affine'], default='affine')\n",
        "\n",
        "    arg_parser.add_argument('--decode_max_time_step', default=40, type=int, help='maximum number of time steps used '\n",
        "                                                                                 'in decoding and sampling')\n",
        "\n",
        "\n",
        "    arg_parser.add_argument('--save_to', default='model', type=str, help='save trained model to')\n",
        "    arg_parser.add_argument('--toy', action='store_true',\n",
        "                            help='If set, use small data; used for fast debugging.')\n",
        "    arg_parser.add_argument('--clip_grad', default=5., type=float, help='clip gradients')\n",
        "    arg_parser.add_argument('--max_epoch', default=-1, type=int, help='maximum number of training epoches')\n",
        "    arg_parser.add_argument('--optimizer', default='Adam', type=str, help='optimizer')\n",
        "    arg_parser.add_argument('--lr', default=0.001, type=float, help='learning rate')\n",
        "\n",
        "    arg_parser.add_argument('--dataset', default=data_path, type=str)\n",
        "\n",
        "    arg_parser.add_argument('--epoch', default=50, type=int, help='Maximum Epoch')\n",
        "    arg_parser.add_argument('--save', default=base_path, type=str, help=\"Path to save the checkpoint and logs of epoch\")\n",
        "    print(\"left argparsw\")\n",
        "    arg_parser.parse_args()\n",
        "\n",
        "    return arg_parser\n",
        "\n",
        "def init_config(arg_parser):\n",
        "    args = arg_parser.parse_args()\n",
        "    print(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.cuda:\n",
        "        torch.cuda.manual_seed(args.seed)\n",
        "    np.random.seed(int(args.seed * 13 / 7))\n",
        "    random.seed(int(args.seed))\n",
        "    return args"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9rWtfJorxQo",
        "outputId": "b2fa5f65-0916-48e1-c0e9-1de46382ca0a"
      },
      "source": [
        "myargs = init_config(init_arg_parser())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inside argparse\n",
            "left argparsw\n",
            "90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ExVc5te6qSsa",
        "outputId": "9eb5cd27-ab3a-4496-dc3e-78cbe183d01e"
      },
      "source": [
        "myargs.dataset+'/predict_lf.json'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/NL2SQL/Models/IRNet-master/data/predict_lf.json'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D86_-I6mt5V0",
        "outputId": "38bfaf7e-275e-434e-d096-964681838ea2"
      },
      "source": [
        "myargs.cuda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "splvDQG5qrwY"
      },
      "source": [
        "def evaluate(args):\n",
        "    \"\"\"\n",
        "    :param args:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    grammar = semQL.Grammar()\n",
        "    sql_data, table_data, val_sql_data,\\\n",
        "    val_table_data= utils.load_dataset(args.dataset, use_small=args.toy)\n",
        "\n",
        "    model = IRNet(args, grammar)\n",
        "\n",
        "    print(\"is cuda: {}\".format(args.cuda))\n",
        "\n",
        "    if args.cuda: \n",
        "      model.cuda()\n",
        "\n",
        "    print('load pretrained model from %s'% (args.load_model))\n",
        "    pretrained_model = torch.load(args.load_model,\n",
        "                                     map_location=lambda storage, loc: storage)\n",
        "    \n",
        "    pretrained_modeled = copy.deepcopy(pretrained_model)\n",
        "    for k in pretrained_model.keys():\n",
        "        if k not in model.state_dict().keys():\n",
        "            del pretrained_modeled[k]\n",
        "\n",
        "    model.load_state_dict(pretrained_modeled)\n",
        "    print(\"Loaded Model\")\n",
        "\n",
        "    model.word_emb = utils.load_word_emb(args.glove_embed_path)\n",
        "    print(\"Loaded Glove\")\n",
        "\n",
        "    json_datas, sketch_acc, acc = utils.epoch_acc(model, args.batch_size, val_sql_data, val_table_data,\n",
        "                           beam_size=args.beam_size)\n",
        "    print('Sketch Acc: %f, Acc: %f' % (sketch_acc, acc))\n",
        "    # utils.eval_acc(json_datas, val_sql_data)\n",
        "    \n",
        "    with open(args.dataset+'/predict_lf.json', 'w') as f:\n",
        "        json.dump(json_datas, f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U48v2X4XjSWK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHHZ2l6gk5VJ"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr5UDXqjRwGa",
        "outputId": "2c16699f-b564-48e8-96cd-ff5cffbf1d99"
      },
      "source": [
        "evaluate(myargs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading from datasets...\n",
            "Loading data from /content/drive/My Drive/NL2SQL/Models/IRNet-master/data/tables.json\n",
            "Loading data from /content/drive/My Drive/NL2SQL/Models/IRNet-master/data/train.json\n",
            "Loading data from /content/drive/My Drive/NL2SQL/Models/IRNet-master/data/dev.json\n",
            "Use Column Pointer:  False\n",
            "is cuda: True\n",
            "load pretrained model from /content/drive/My Drive/NL2SQL/Models/IRNet-master/saved_model/IRNet_pretrained.model\n",
            "Loaded Model\n",
            "Loading word embedding from /content/drive/My Drive/NL2SQL/Models/IRNet-master/data/glove.42B.300d.txt\n",
            "Loaded Glove\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Sketch Acc: 0.777237, Acc: 0.469844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w2VQKGDRwNx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQeDhrbik6eI"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap71vFuoRwPq"
      },
      "source": [
        "def split_logical_form(lf):\n",
        "    indexs = [i+1 for i, letter in enumerate(lf) if letter == ')']\n",
        "    indexs.insert(0, 0)\n",
        "    components = list()\n",
        "    for i in range(1, len(indexs)):\n",
        "        components.append(lf[indexs[i-1]:indexs[i]].strip())\n",
        "    return components\n",
        "\n",
        "\n",
        "def pop_front(array):\n",
        "    if len(array) == 0:\n",
        "        return 'None'\n",
        "    return array.pop(0)\n",
        "\n",
        "\n",
        "def is_end(components, transformed_sql, is_root_processed):\n",
        "    end = False\n",
        "    c = pop_front(components)\n",
        "    c_instance = eval(c)\n",
        "\n",
        "    if isinstance(c_instance, Root) and is_root_processed:\n",
        "        # intersect, union, except\n",
        "        end = True\n",
        "    elif isinstance(c_instance, Filter):\n",
        "        if 'where' not in transformed_sql:\n",
        "            end = True\n",
        "        else:\n",
        "            num_conjunction = 0\n",
        "            for f in transformed_sql['where']:\n",
        "                if isinstance(f, str) and (f == 'and' or f == 'or'):\n",
        "                    num_conjunction += 1\n",
        "            current_filters = len(transformed_sql['where'])\n",
        "            valid_filters = current_filters - num_conjunction\n",
        "            if valid_filters >= num_conjunction + 1:\n",
        "                end = True\n",
        "    elif isinstance(c_instance, Order):\n",
        "        if 'order' not in transformed_sql:\n",
        "            end = True\n",
        "        elif len(transformed_sql['order']) == 0:\n",
        "            end = False\n",
        "        else:\n",
        "            end = True\n",
        "    elif isinstance(c_instance, Sup):\n",
        "        if 'sup' not in transformed_sql:\n",
        "            end = True\n",
        "        elif len(transformed_sql['sup']) == 0:\n",
        "            end = False\n",
        "        else:\n",
        "            end = True\n",
        "    components.insert(0, c)\n",
        "    return end\n",
        "\n",
        "\n",
        "def _transform(components, transformed_sql, col_set, table_names, schema):\n",
        "    processed_root = False\n",
        "    current_table = schema\n",
        "\n",
        "    while len(components) > 0:\n",
        "        if is_end(components, transformed_sql, processed_root):\n",
        "            break\n",
        "        c = pop_front(components)\n",
        "        c_instance = eval(c)\n",
        "        if isinstance(c_instance, Root):\n",
        "            processed_root = True\n",
        "            transformed_sql['select'] = list()\n",
        "            if c_instance.id_c == 0:\n",
        "                transformed_sql['where'] = list()\n",
        "                transformed_sql['sup'] = list()\n",
        "            elif c_instance.id_c == 1:\n",
        "                transformed_sql['where'] = list()\n",
        "                transformed_sql['order'] = list()\n",
        "            elif c_instance.id_c == 2:\n",
        "                transformed_sql['sup'] = list()\n",
        "            elif c_instance.id_c == 3:\n",
        "                transformed_sql['where'] = list()\n",
        "            elif c_instance.id_c == 4:\n",
        "                transformed_sql['order'] = list()\n",
        "        elif isinstance(c_instance, Sel):\n",
        "            continue\n",
        "        elif isinstance(c_instance, N):\n",
        "            for i in range(c_instance.id_c + 1):\n",
        "                agg = eval(pop_front(components))\n",
        "                column = eval(pop_front(components))\n",
        "                _table = pop_front(components)\n",
        "                table = eval(_table)\n",
        "                if not isinstance(table, T):\n",
        "                    table = None\n",
        "                    components.insert(0, _table)\n",
        "                assert isinstance(agg, A) and isinstance(column, C)\n",
        "\n",
        "                transformed_sql['select'].append((\n",
        "                    agg.production.split()[1],\n",
        "                    replace_col_with_original_col(col_set[column.id_c], table_names[table.id_c], current_table) if table is not None else col_set[column.id_c],\n",
        "                    table_names[table.id_c] if table is not None else table\n",
        "                ))\n",
        "\n",
        "        elif isinstance(c_instance, Sup):\n",
        "            transformed_sql['sup'].append(c_instance.production.split()[1])\n",
        "            agg = eval(pop_front(components))\n",
        "            column = eval(pop_front(components))\n",
        "            _table = pop_front(components)\n",
        "            table = eval(_table)\n",
        "            if not isinstance(table, T):\n",
        "                table = None\n",
        "                components.insert(0, _table)\n",
        "            assert isinstance(agg, A) and isinstance(column, C)\n",
        "\n",
        "            transformed_sql['sup'].append(agg.production.split()[1])\n",
        "            if table:\n",
        "                fix_col_id = replace_col_with_original_col(col_set[column.id_c], table_names[table.id_c], current_table)\n",
        "            else:\n",
        "                fix_col_id = col_set[column.id_c]\n",
        "                raise RuntimeError('not found table !!!!')\n",
        "            transformed_sql['sup'].append(fix_col_id)\n",
        "            transformed_sql['sup'].append(table_names[table.id_c] if table is not None else table)\n",
        "\n",
        "        elif isinstance(c_instance, Order):\n",
        "            transformed_sql['order'].append(c_instance.production.split()[1])\n",
        "            agg = eval(pop_front(components))\n",
        "            column = eval(pop_front(components))\n",
        "            _table = pop_front(components)\n",
        "            table = eval(_table)\n",
        "            if not isinstance(table, T):\n",
        "                table = None\n",
        "                components.insert(0, _table)\n",
        "            assert isinstance(agg, A) and isinstance(column, C)\n",
        "            transformed_sql['order'].append(agg.production.split()[1])\n",
        "            transformed_sql['order'].append(replace_col_with_original_col(col_set[column.id_c], table_names[table.id_c], current_table))\n",
        "            transformed_sql['order'].append(table_names[table.id_c] if table is not None else table)\n",
        "\n",
        "        elif isinstance(c_instance, Filter):\n",
        "            op = c_instance.production.split()[1]\n",
        "            if op == 'and' or op == 'or':\n",
        "                transformed_sql['where'].append(op)\n",
        "            else:\n",
        "                # No Supquery\n",
        "                agg = eval(pop_front(components))\n",
        "                column = eval(pop_front(components))\n",
        "                _table = pop_front(components)\n",
        "                table = eval(_table)\n",
        "                if not isinstance(table, T):\n",
        "                    table = None\n",
        "                    components.insert(0, _table)\n",
        "                assert isinstance(agg, A) and isinstance(column, C)\n",
        "                if len(c_instance.production.split()) == 3:\n",
        "                    if table:\n",
        "                        fix_col_id = replace_col_with_original_col(col_set[column.id_c], table_names[table.id_c], current_table)\n",
        "                    else:\n",
        "                        fix_col_id = col_set[column.id_c]\n",
        "                        raise RuntimeError('not found table !!!!')\n",
        "                    transformed_sql['where'].append((\n",
        "                        op,\n",
        "                        agg.production.split()[1],\n",
        "                        fix_col_id,\n",
        "                        table_names[table.id_c] if table is not None else table,\n",
        "                        None\n",
        "                    ))\n",
        "                else:\n",
        "                    # Subquery\n",
        "                    new_dict = dict()\n",
        "                    new_dict['sql'] = transformed_sql['sql']\n",
        "                    transformed_sql['where'].append((\n",
        "                        op,\n",
        "                        agg.production.split()[1],\n",
        "                        replace_col_with_original_col(col_set[column.id_c], table_names[table.id_c], current_table),\n",
        "                        table_names[table.id_c] if table is not None else table,\n",
        "                        _transform(components, new_dict, col_set, table_names, schema)\n",
        "                    ))\n",
        "\n",
        "    return transformed_sql\n",
        "\n",
        "\n",
        "def transform(query, schema, origin=None):\n",
        "    preprocess_schema(schema)\n",
        "    if origin is None:\n",
        "        lf = query['model_result_replace']\n",
        "    else:\n",
        "        lf = origin\n",
        "    # lf = query['rule_label']\n",
        "    col_set = query['col_set']\n",
        "    table_names = query['table_names']\n",
        "    current_table = schema\n",
        "\n",
        "    current_table['schema_content_clean'] = [x[1] for x in current_table['column_names']]\n",
        "    current_table['schema_content'] = [x[1] for x in current_table['column_names_original']]\n",
        "\n",
        "    components = split_logical_form(lf)\n",
        "\n",
        "    transformed_sql = dict()\n",
        "    transformed_sql['sql'] = query\n",
        "    c = pop_front(components)\n",
        "    c_instance = eval(c)\n",
        "    assert isinstance(c_instance, Root1)\n",
        "    if c_instance.id_c == 0:\n",
        "        transformed_sql['intersect'] = dict()\n",
        "        transformed_sql['intersect']['sql'] = query\n",
        "\n",
        "        _transform(components, transformed_sql, col_set, table_names, schema)\n",
        "        _transform(components, transformed_sql['intersect'], col_set, table_names, schema)\n",
        "    elif c_instance.id_c == 1:\n",
        "        transformed_sql['union'] = dict()\n",
        "        transformed_sql['union']['sql'] = query\n",
        "        _transform(components, transformed_sql, col_set, table_names, schema)\n",
        "        _transform(components, transformed_sql['union'], col_set, table_names, schema)\n",
        "    elif c_instance.id_c == 2:\n",
        "        transformed_sql['except'] = dict()\n",
        "        transformed_sql['except']['sql'] = query\n",
        "        _transform(components, transformed_sql, col_set, table_names, schema)\n",
        "        _transform(components, transformed_sql['except'], col_set, table_names, schema)\n",
        "    else:\n",
        "        _transform(components, transformed_sql, col_set, table_names, schema)\n",
        "\n",
        "    parse_result = to_str(transformed_sql, 1, schema)\n",
        "\n",
        "    parse_result = parse_result.replace('\\t', '')\n",
        "    return [parse_result]\n",
        "\n",
        "def col_to_str(agg, col, tab, table_names, N=1):\n",
        "    _col = col.replace(' ', '_')\n",
        "    if agg == 'none':\n",
        "        if tab not in table_names:\n",
        "            table_names[tab] = 'T' + str(len(table_names) + N)\n",
        "        table_alias = table_names[tab]\n",
        "        if col == '*':\n",
        "            return '*'\n",
        "        return '%s.%s' % (table_alias, _col)\n",
        "    else:\n",
        "        if col == '*':\n",
        "            if tab is not None and tab not in table_names:\n",
        "                table_names[tab] = 'T' + str(len(table_names) + N)\n",
        "            return '%s(%s)' % (agg, _col)\n",
        "        else:\n",
        "            if tab not in table_names:\n",
        "                table_names[tab] = 'T' + str(len(table_names) + N)\n",
        "            table_alias = table_names[tab]\n",
        "            return '%s(%s.%s)' % (agg, table_alias, _col)\n",
        "\n",
        "\n",
        "def infer_from_clause(table_names, schema, columns):\n",
        "    tables = list(table_names.keys())\n",
        "    # print(table_names)\n",
        "    start_table = None\n",
        "    end_table = None\n",
        "    join_clause = list()\n",
        "    if len(tables) == 1:\n",
        "        join_clause.append((tables[0], table_names[tables[0]]))\n",
        "    elif len(tables) == 2:\n",
        "        use_graph = True\n",
        "        # print(schema['graph'].vertices)\n",
        "        for t in tables:\n",
        "            if t not in schema['graph'].vertices:\n",
        "                use_graph = False\n",
        "                break\n",
        "        if use_graph:\n",
        "            start_table = tables[0]\n",
        "            end_table = tables[1]\n",
        "            _tables = list(schema['graph'].dijkstra(tables[0], tables[1]))\n",
        "            # print('Two tables: ', _tables)\n",
        "            max_key = 1\n",
        "            for t, k in table_names.items():\n",
        "                _k = int(k[1:])\n",
        "                if _k > max_key:\n",
        "                    max_key = _k\n",
        "            for t in _tables:\n",
        "                if t not in table_names:\n",
        "                    table_names[t] = 'T' + str(max_key + 1)\n",
        "                    max_key += 1\n",
        "                join_clause.append((t, table_names[t],))\n",
        "        else:\n",
        "            join_clause = list()\n",
        "            for t in tables:\n",
        "                join_clause.append((t, table_names[t],))\n",
        "    else:\n",
        "        # > 2\n",
        "        # print('More than 2 table')\n",
        "        for t in tables:\n",
        "            join_clause.append((t, table_names[t],))\n",
        "\n",
        "    if len(join_clause) >= 3:\n",
        "        star_table = None\n",
        "        for agg, col, tab in columns:\n",
        "            if col == '*':\n",
        "                star_table = tab\n",
        "                break\n",
        "        if star_table is not None:\n",
        "            star_table_count = 0\n",
        "            for agg, col, tab in columns:\n",
        "                if tab == star_table and col != '*':\n",
        "                    star_table_count += 1\n",
        "            if star_table_count == 0 and ((end_table is None or end_table == star_table) or (start_table is None or start_table == star_table)):\n",
        "                # Remove the table the rest tables still can join without star_table\n",
        "                new_join_clause = list()\n",
        "                for t in join_clause:\n",
        "                    if t[0] != star_table:\n",
        "                        new_join_clause.append(t)\n",
        "                join_clause = new_join_clause\n",
        "\n",
        "    join_clause = ' JOIN '.join(['%s AS %s' % (jc[0], jc[1]) for jc in join_clause])\n",
        "    return 'FROM ' + join_clause\n",
        "\n",
        "def replace_col_with_original_col(query, col, current_table):\n",
        "    # print(query, col)\n",
        "    if query == '*':\n",
        "        return query\n",
        "\n",
        "    cur_table = col\n",
        "    cur_col = query\n",
        "    single_final_col = None\n",
        "    # print(query, col)\n",
        "    for col_ind, col_name in enumerate(current_table['schema_content_clean']):\n",
        "        if col_name == cur_col:\n",
        "            assert cur_table in current_table['table_names']\n",
        "            if current_table['table_names'][current_table['col_table'][col_ind]] == cur_table:\n",
        "                single_final_col = current_table['column_names_original'][col_ind][1]\n",
        "                break\n",
        "\n",
        "    assert single_final_col\n",
        "    # if query != single_final_col:\n",
        "    #     print(query, single_final_col)\n",
        "    return single_final_col\n",
        "\n",
        "\n",
        "def build_graph(schema):\n",
        "    relations = list()\n",
        "    foreign_keys = schema['foreign_keys']\n",
        "    for (fkey, pkey) in foreign_keys:\n",
        "        fkey_table = schema['table_names_original'][schema['column_names'][fkey][0]]\n",
        "        pkey_table = schema['table_names_original'][schema['column_names'][pkey][0]]\n",
        "        relations.append((fkey_table, pkey_table))\n",
        "        relations.append((pkey_table, fkey_table))\n",
        "    return Graph(relations)\n",
        "\n",
        "\n",
        "def preprocess_schema(schema):\n",
        "    tmp_col = []\n",
        "    for cc in [x[1] for x in schema['column_names']]:\n",
        "        if cc not in tmp_col:\n",
        "            tmp_col.append(cc)\n",
        "    schema['col_set'] = tmp_col\n",
        "    # print table\n",
        "    schema['schema_content'] = [col[1] for col in schema['column_names']]\n",
        "    schema['col_table'] = [col[0] for col in schema['column_names']]\n",
        "    graph = build_graph(schema)\n",
        "    schema['graph'] = graph\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def to_str(sql_json, N_T, schema, pre_table_names=None):\n",
        "    all_columns = list()\n",
        "    select_clause = list()\n",
        "    table_names = dict()\n",
        "    current_table = schema\n",
        "    for (agg, col, tab) in sql_json['select']:\n",
        "        all_columns.append((agg, col, tab))\n",
        "        select_clause.append(col_to_str(agg, col, tab, table_names, N_T))\n",
        "    select_clause_str = 'SELECT ' + ', '.join(select_clause).strip()\n",
        "\n",
        "    sup_clause = ''\n",
        "    order_clause = ''\n",
        "    direction_map = {\"des\": 'DESC', 'asc': 'ASC'}\n",
        "\n",
        "    if 'sup' in sql_json:\n",
        "        (direction, agg, col, tab,) = sql_json['sup']\n",
        "        all_columns.append((agg, col, tab))\n",
        "        subject = col_to_str(agg, col, tab, table_names, N_T)\n",
        "        sup_clause = ('ORDER BY %s %s LIMIT 1' % (subject, direction_map[direction])).strip()\n",
        "    elif 'order' in sql_json:\n",
        "        (direction, agg, col, tab,) = sql_json['order']\n",
        "        all_columns.append((agg, col, tab))\n",
        "        subject = col_to_str(agg, col, tab, table_names, N_T)\n",
        "        order_clause = ('ORDER BY %s %s' % (subject, direction_map[direction])).strip()\n",
        "\n",
        "    has_group_by = False\n",
        "    where_clause = ''\n",
        "    have_clause = ''\n",
        "    if 'where' in sql_json:\n",
        "        conjunctions = list()\n",
        "        filters = list()\n",
        "        # print(sql_json['where'])\n",
        "        for f in sql_json['where']:\n",
        "            if isinstance(f, str):\n",
        "                conjunctions.append(f)\n",
        "            else:\n",
        "                op, agg, col, tab, value = f\n",
        "                if value:\n",
        "                    value['sql'] = sql_json['sql']\n",
        "                all_columns.append((agg, col, tab))\n",
        "                subject = col_to_str(agg, col, tab, table_names, N_T)\n",
        "                if value is None:\n",
        "                    where_value = '1'\n",
        "                    if op == 'between':\n",
        "                        where_value = '1 AND 2'\n",
        "                    filters.append('%s %s %s' % (subject, op, where_value))\n",
        "                else:\n",
        "                    if op == 'in' and len(value['select']) == 1 and value['select'][0][0] == 'none' \\\n",
        "                            and 'where' not in value and 'order' not in value and 'sup' not in value:\n",
        "                            # and value['select'][0][2] not in table_names:\n",
        "                        if value['select'][0][2] not in table_names:\n",
        "                            table_names[value['select'][0][2]] = 'T' + str(len(table_names) + N_T)\n",
        "                        filters.append(None)\n",
        "\n",
        "                    else:\n",
        "                        filters.append('%s %s %s' % (subject, op, '(' + to_str(value, len(table_names) + 1, schema) + ')'))\n",
        "                if len(conjunctions):\n",
        "                    filters.append(conjunctions.pop())\n",
        "\n",
        "        aggs = ['count(', 'avg(', 'min(', 'max(', 'sum(']\n",
        "        having_filters = list()\n",
        "        idx = 0\n",
        "        while idx < len(filters):\n",
        "            _filter = filters[idx]\n",
        "            if _filter is None:\n",
        "                idx += 1\n",
        "                continue\n",
        "            for agg in aggs:\n",
        "                if _filter.startswith(agg):\n",
        "                    having_filters.append(_filter)\n",
        "                    filters.pop(idx)\n",
        "                    # print(filters)\n",
        "                    if 0 < idx and (filters[idx - 1] in ['and', 'or']):\n",
        "                        filters.pop(idx - 1)\n",
        "                        # print(filters)\n",
        "                    break\n",
        "            else:\n",
        "                idx += 1\n",
        "        if len(having_filters) > 0:\n",
        "            have_clause = 'HAVING ' + ' '.join(having_filters).strip()\n",
        "        if len(filters) > 0:\n",
        "            # print(filters)\n",
        "            filters = [_f for _f in filters if _f is not None]\n",
        "            conjun_num = 0\n",
        "            filter_num = 0\n",
        "            for _f in filters:\n",
        "                if _f in ['or', 'and']:\n",
        "                    conjun_num += 1\n",
        "                else:\n",
        "                    filter_num += 1\n",
        "            if conjun_num > 0 and filter_num != (conjun_num + 1):\n",
        "                # assert 'and' in filters\n",
        "                idx = 0\n",
        "                while idx < len(filters):\n",
        "                    if filters[idx] == 'and':\n",
        "                        if idx - 1 == 0:\n",
        "                            filters.pop(idx)\n",
        "                            break\n",
        "                        if filters[idx - 1] in ['and', 'or']:\n",
        "                            filters.pop(idx)\n",
        "                            break\n",
        "                        if idx + 1 >= len(filters) - 1:\n",
        "                            filters.pop(idx)\n",
        "                            break\n",
        "                        if filters[idx + 1] in ['and', 'or']:\n",
        "                            filters.pop(idx)\n",
        "                            break\n",
        "                    idx += 1\n",
        "            if len(filters) > 0:\n",
        "                where_clause = 'WHERE ' + ' '.join(filters).strip()\n",
        "                where_clause = where_clause.replace('not_in', 'NOT IN')\n",
        "            else:\n",
        "                where_clause = ''\n",
        "\n",
        "        if len(having_filters) > 0:\n",
        "            has_group_by = True\n",
        "\n",
        "    for agg in ['count(', 'avg(', 'min(', 'max(', 'sum(']:\n",
        "        if (len(sql_json['select']) > 1 and agg in select_clause_str)\\\n",
        "                or agg in sup_clause or agg in order_clause:\n",
        "            has_group_by = True\n",
        "            break\n",
        "\n",
        "    group_by_clause = ''\n",
        "    if has_group_by:\n",
        "        if len(table_names) == 1:\n",
        "            # check none agg\n",
        "            is_agg_flag = False\n",
        "            for (agg, col, tab) in sql_json['select']:\n",
        "\n",
        "                if agg == 'none':\n",
        "                    group_by_clause = 'GROUP BY ' + col_to_str(agg, col, tab, table_names, N_T)\n",
        "                else:\n",
        "                    is_agg_flag = True\n",
        "\n",
        "            if is_agg_flag is False and len(group_by_clause) > 5:\n",
        "                group_by_clause = \"GROUP BY\"\n",
        "                for (agg, col, tab) in sql_json['select']:\n",
        "                    group_by_clause = group_by_clause + ' ' + col_to_str(agg, col, tab, table_names, N_T)\n",
        "\n",
        "            if len(group_by_clause) < 5:\n",
        "                if 'count(*)' in select_clause_str:\n",
        "                    current_table = schema\n",
        "                    for primary in current_table['primary_keys']:\n",
        "                        if current_table['table_names'][current_table['col_table'][primary]] in table_names :\n",
        "                            group_by_clause = 'GROUP BY ' + col_to_str('none', current_table['schema_content'][primary],\n",
        "                                                                       current_table['table_names'][\n",
        "                                                                           current_table['col_table'][primary]],\n",
        "                                                                       table_names, N_T)\n",
        "        else:\n",
        "            # if only one select\n",
        "            if len(sql_json['select']) == 1:\n",
        "                agg, col, tab = sql_json['select'][0]\n",
        "                non_lists = [tab]\n",
        "                fix_flag = False\n",
        "                # add tab from other part\n",
        "                for key, value in table_names.items():\n",
        "                    if key not in non_lists:\n",
        "                        non_lists.append(key)\n",
        "\n",
        "                a = non_lists[0]\n",
        "                b = None\n",
        "                for non in non_lists:\n",
        "                    if a != non:\n",
        "                        b = non\n",
        "                if b:\n",
        "                    for pair in current_table['foreign_keys']:\n",
        "                        t1 = current_table['table_names'][current_table['col_table'][pair[0]]]\n",
        "                        t2 = current_table['table_names'][current_table['col_table'][pair[1]]]\n",
        "                        if t1 in [a, b] and t2 in [a, b]:\n",
        "                            if pre_table_names and t1 not in pre_table_names:\n",
        "                                assert t2 in pre_table_names\n",
        "                                t1 = t2\n",
        "                            group_by_clause = 'GROUP BY ' + col_to_str('none',\n",
        "                                                                       current_table['schema_content'][pair[0]],\n",
        "                                                                       t1,\n",
        "                                                                       table_names, N_T)\n",
        "                            fix_flag = True\n",
        "                            break\n",
        "\n",
        "                if fix_flag is False:\n",
        "                    agg, col, tab = sql_json['select'][0]\n",
        "                    group_by_clause = 'GROUP BY ' + col_to_str(agg, col, tab, table_names, N_T)\n",
        "\n",
        "            else:\n",
        "                # check if there are only one non agg\n",
        "                non_agg, non_agg_count = None, 0\n",
        "                non_lists = []\n",
        "                for (agg, col, tab) in sql_json['select']:\n",
        "                    if agg == 'none':\n",
        "                        non_agg = (agg, col, tab)\n",
        "                        non_lists.append(tab)\n",
        "                        non_agg_count += 1\n",
        "\n",
        "                non_lists = list(set(non_lists))\n",
        "                # print(non_lists)\n",
        "                if non_agg_count == 1:\n",
        "                    group_by_clause = 'GROUP BY ' + col_to_str(non_agg[0], non_agg[1], non_agg[2], table_names, N_T)\n",
        "                elif non_agg:\n",
        "                    find_flag = False\n",
        "                    fix_flag = False\n",
        "                    find_primary = None\n",
        "                    if len(non_lists) <= 1:\n",
        "                        for key, value in table_names.items():\n",
        "                            if key not in non_lists:\n",
        "                                non_lists.append(key)\n",
        "                    if len(non_lists) > 1:\n",
        "                        a = non_lists[0]\n",
        "                        b = None\n",
        "                        for non in non_lists:\n",
        "                            if a != non:\n",
        "                                b = non\n",
        "                        if b:\n",
        "                            for pair in current_table['foreign_keys']:\n",
        "                                t1 = current_table['table_names'][current_table['col_table'][pair[0]]]\n",
        "                                t2 = current_table['table_names'][current_table['col_table'][pair[1]]]\n",
        "                                if t1 in [a, b] and t2 in [a, b]:\n",
        "                                    if pre_table_names and t1 not in pre_table_names:\n",
        "                                        assert  t2 in pre_table_names\n",
        "                                        t1 = t2\n",
        "                                    group_by_clause = 'GROUP BY ' + col_to_str('none',\n",
        "                                                                               current_table['schema_content'][pair[0]],\n",
        "                                                                               t1,\n",
        "                                                                               table_names, N_T)\n",
        "                                    fix_flag = True\n",
        "                                    break\n",
        "                    tab = non_agg[2]\n",
        "                    assert tab in current_table['table_names']\n",
        "\n",
        "                    for primary in current_table['primary_keys']:\n",
        "                        if current_table['table_names'][current_table['col_table'][primary]] == tab:\n",
        "                            find_flag = True\n",
        "                            find_primary = (current_table['schema_content'][primary], tab)\n",
        "                    if fix_flag is False:\n",
        "                        if find_flag is False:\n",
        "                            # rely on count *\n",
        "                            foreign = []\n",
        "                            for pair in current_table['foreign_keys']:\n",
        "                                if current_table['table_names'][current_table['col_table'][pair[0]]] == tab:\n",
        "                                    foreign.append(pair[1])\n",
        "                                if current_table['table_names'][current_table['col_table'][pair[1]]] == tab:\n",
        "                                    foreign.append(pair[0])\n",
        "\n",
        "                            for pair in foreign:\n",
        "                                if current_table['table_names'][current_table['col_table'][pair]] in table_names:\n",
        "                                    group_by_clause = 'GROUP BY ' + col_to_str('none', current_table['schema_content'][pair],\n",
        "                                                                                   current_table['table_names'][current_table['col_table'][pair]],\n",
        "                                                                                   table_names, N_T)\n",
        "                                    find_flag = True\n",
        "                                    break\n",
        "                            if find_flag is False:\n",
        "                                for (agg, col, tab) in sql_json['select']:\n",
        "                                    if 'id' in col.lower():\n",
        "                                        group_by_clause = 'GROUP BY ' + col_to_str(agg, col, tab, table_names, N_T)\n",
        "                                        break\n",
        "                                if len(group_by_clause) > 5:\n",
        "                                    pass\n",
        "                                else:\n",
        "                                    raise RuntimeError('fail to convert')\n",
        "                        else:\n",
        "                            group_by_clause = 'GROUP BY ' + col_to_str('none', find_primary[0],\n",
        "                                                                       find_primary[1],\n",
        "                                                                       table_names, N_T)\n",
        "    intersect_clause = ''\n",
        "    if 'intersect' in sql_json:\n",
        "        sql_json['intersect']['sql'] = sql_json['sql']\n",
        "        intersect_clause = 'INTERSECT ' + to_str(sql_json['intersect'], len(table_names) + 1, schema, table_names)\n",
        "    union_clause = ''\n",
        "    if 'union' in sql_json:\n",
        "        sql_json['union']['sql'] = sql_json['sql']\n",
        "        union_clause = 'UNION ' + to_str(sql_json['union'], len(table_names) + 1, schema, table_names)\n",
        "    except_clause = ''\n",
        "    if 'except' in sql_json:\n",
        "        sql_json['except']['sql'] = sql_json['sql']\n",
        "        except_clause = 'EXCEPT ' + to_str(sql_json['except'], len(table_names) + 1, schema, table_names)\n",
        "\n",
        "    # print(current_table['table_names_original'])\n",
        "    table_names_replace = {}\n",
        "    for a, b in zip(current_table['table_names_original'], current_table['table_names']):\n",
        "        table_names_replace[b] = a\n",
        "    new_table_names = {}\n",
        "    for key, value in table_names.items():\n",
        "        if key is None:\n",
        "            continue\n",
        "        new_table_names[table_names_replace[key]] = value\n",
        "    from_clause = infer_from_clause(new_table_names, schema, all_columns).strip()\n",
        "\n",
        "    sql = ' '.join([select_clause_str, from_clause, where_clause, group_by_clause, have_clause, sup_clause, order_clause,\n",
        "                    intersect_clause, union_clause, except_clause])\n",
        "\n",
        "    return sql\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH66AuBKuEyq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1ZdX-eglmXB"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NByeAG0Flmzd"
      },
      "source": [
        "import argparse\n",
        "import traceback\n",
        "\n",
        "from src.rule.graph import Graph\n",
        "from src.rule.semQL import Sup, Sel, Order, Root, Filter, A, N, C, T, Root1\n",
        "from src.rule.sem_utils import alter_inter, alter_not_in, alter_column0, load_dataSets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l91ZBPqYlm2Q",
        "outputId": "9df1e3ed-2e42-4cd5-c73f-87d53f43d656"
      },
      "source": [
        "arg_parserx = argparse.ArgumentParser()\n",
        "arg_parserx.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
        "arg_parserx.add_argument('--data_path', default=data_path, type=str, help='dataset path')\n",
        "arg_parserx.add_argument('--input_path', default=\"predict_lf.json\", type=str, help='predicted logical form')\n",
        "arg_parserx.add_argument('--output_path', default=\"./output_sql.sql\", type=str, help='output data')\n",
        "print(\"--\")\n",
        "myargssemql = arg_parserx.parse_args()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dwFhnD0plm40",
        "outputId": "2c9b55c1-c303-4387-d6c8-309f0d8551e8"
      },
      "source": [
        "myargssemql.data_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/NL2SQL/Models/IRNet-master/data'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "ck-_WYcslm7U",
        "outputId": "ffb11844-63b4-493b-a22b-84dd47745114"
      },
      "source": [
        "# loading dataSets\n",
        "datas, schemas = load_dataSets(myargssemql)\n",
        "alter_not_in(datas, schemas=schemas)\n",
        "alter_inter(datas)\n",
        "alter_column0(datas)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pattern/text/__init__.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(path, encoding, comment)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStopIteration\u001b[0m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-5df7edb4b4c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# loading dataSets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdatas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschemas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataSets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyargssemql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0malter_not_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschemas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschemas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0malter_inter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0malter_column0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/NL2SQL/Models/IRNet-master/src/rule/sem_utils.py\u001b[0m in \u001b[0;36malter_not_in\u001b[0;34m(datas, schemas)\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mh_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin_table_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_arg_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlabel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/NL2SQL/Models/IRNet-master/src/rule/sem_utils.py\u001b[0m in \u001b[0;36mfind_table\u001b[0;34m(cur_table, origin_table_names, question_arg_type, question_arg)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mquestion_arg_type\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'NONE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin_table_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mpartial_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_arg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt_id\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mh_table\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mt_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/NL2SQL/Models/IRNet-master/src/rule/sem_utils.py\u001b[0m in \u001b[0;36mpartial_match\u001b[0;34m(query, table_name)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpartial_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mtable_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/NL2SQL/Models/IRNet-master/src/rule/sem_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpartial_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mtable_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pattern/text/__init__.py\u001b[0m in \u001b[0;36mlemma\u001b[0;34m(self, verb, parse)\u001b[0m\n\u001b[1;32m   2170\u001b[0m         \"\"\"\n\u001b[1;32m   2171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2172\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inverse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2174\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inverse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mverb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pattern/text/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2125\u001b[0m         \u001b[0;31m# have,,,has,,having,,,,,had,had,haven't,,,hasn't,,,,,,,hadn't,hadn't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2126\u001b[0m         \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTENSES_ID\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mINFINITIVE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2127\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2128\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m             \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: generator raised StopIteration"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcPGam7-lm98"
      },
      "source": [
        "index = range(len(datas))\n",
        "count = 0\n",
        "exception_count = 0\n",
        "with open(myargssemql.output_path, 'w', encoding='utf8') as d, open('gold.txt', 'w', encoding='utf8') as g:\n",
        "  for i in index:\n",
        "    try:\n",
        "      result = transform(datas[i], schemas[datas[i]['db_id']])\n",
        "      d.write(result[0] + '\\n')\n",
        "      g.write(\"%s\\t%s\\t%s\\n\" % (datas[i]['query'], datas[i][\"db_id\"], datas[i][\"question\"]))\n",
        "      count += 1\n",
        "    except Exception as e:\n",
        "      result = transform(datas[i], schemas[datas[i]['db_id']], origin='Root1(3) Root(5) Sel(0) N(0) A(3) C(0) T(0)')\n",
        "      exception_count += 1\n",
        "      d.write(result[0] + '\\n')\n",
        "      g.write(\"%s\\t%s\\t%s\\n\" % (datas[i]['query'], datas[i][\"db_id\"], datas[i][\"question\"]))\n",
        "      count += 1\n",
        "      print(e)\n",
        "      print('Exception')\n",
        "      print(traceback.format_exc())\n",
        "      print('===\\n\\n')\n",
        "\n",
        "print(count, exception_count)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}